{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce7d96e1",
   "metadata": {},
   "source": [
    "# Lesson 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32e83c5",
   "metadata": {},
   "source": [
    "## Step 1: Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85123f8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-17T13:21:31.539189Z",
     "start_time": "2022-09-17T13:21:31.527804Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import Sigmoid\n",
    "from torch.nn import Softmax\n",
    "from torch.nn import Module\n",
    "from torch.optim import SGD\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn.init import kaiming_uniform_\n",
    "from torch.nn.init import xavier_uniform_\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f9cec2",
   "metadata": {},
   "source": [
    "### DATASETS & DATALOADERS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c9187d",
   "metadata": {},
   "source": [
    "Code for processing data samples can get messy and hard to maintain; we ideally want our dataset code to be decoupled from our model training code for better readability and modularity. PyTorch provides two data primitives: torch.utils.data.DataLoader and torch.utils.data.Dataset that allow you to use pre-loaded datasets as well as your own data. Dataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset to enable easy access to the samples.\n",
    "\n",
    "More info: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2880d594",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-17T13:21:31.568399Z",
     "start_time": "2022-09-17T13:21:31.542367Z"
    }
   },
   "outputs": [],
   "source": [
    "class CSVDataset(Dataset):\n",
    "    # load the dataset\n",
    "    def __init__(self, path):\n",
    "        df = pd.read_csv(path)\n",
    "        # store the inputs and outputs\n",
    "        self.X = df.values[:, :-1]\n",
    "        self.y = df.values[:, -1]\n",
    "        # ensure input data is floats\n",
    "        self.X = self.X.astype('float32')\n",
    "        self.y = LabelEncoder().fit_transform(self.y)\n",
    " \n",
    "    # number of rows in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    " \n",
    "    # get a row at an index\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.X[idx], self.y[idx]]\n",
    " \n",
    "    # get indexes for train and test rows\n",
    "    def get_splits(self, n_test=0.2):\n",
    "        # determine sizes\n",
    "        test_size = round(n_test * len(self.X))\n",
    "        train_size = len(self.X) - test_size\n",
    "        # calculate the split\n",
    "        return random_split(self, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a84552f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-17T13:21:31.584393Z",
     "start_time": "2022-09-17T13:21:31.572394Z"
    }
   },
   "outputs": [],
   "source": [
    "# prepare the dataset\n",
    "def prepare_data(path):\n",
    "    # load the dataset\n",
    "    dataset = CSVDataset(path)\n",
    "    # calculate split\n",
    "    train, test = dataset.get_splits()\n",
    "    # prepare data loaders\n",
    "    train_dl = DataLoader(train,batch_size=32, shuffle=True)\n",
    "    test_dl = DataLoader(test,batch_size=32, shuffle=False)\n",
    "    return train_dl, test_dl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d41e87",
   "metadata": {},
   "source": [
    "## Step 2: Prepearing module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a7e60e",
   "metadata": {},
   "source": [
    "### Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdd633c",
   "metadata": {},
   "source": [
    "Pytorch uses modules to represent neural networks: https://pytorch.org/docs/stable/generated/torch.nn.Module.html\n",
    "\n",
    "Xavier and kaiming weight initialization: https://pytorch.org/docs/stable/nn.init.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a1582b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-17T13:21:31.611817Z",
     "start_time": "2022-09-17T13:21:31.584393Z"
    }
   },
   "outputs": [],
   "source": [
    "# model definition\n",
    "class MLP(Module):\n",
    "    # define model elements\n",
    "    def __init__(self, n_inputs):\n",
    "        super(MLP, self).__init__()\n",
    "        # input to first hidden layer\n",
    "        self.hidden1 = Linear(n_inputs, 10)\n",
    "        kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')\n",
    "        self.act1 = ReLU()\n",
    "        # second hidden layer\n",
    "        self.hidden2 = Linear(10, 8)\n",
    "        kaiming_uniform_(self.hidden2.weight, nonlinearity='relu')\n",
    "        self.act2 = ReLU()\n",
    "        # third hidden layer and output\n",
    "        self.hidden3 = Linear(8, 2)\n",
    "        xavier_uniform_(self.hidden3.weight)\n",
    "        self.act3 = Softmax(dim = 1)\n",
    " \n",
    "    # forward propagate input\n",
    "    def forward(self, X):\n",
    "        # input to first hidden layer\n",
    "        X = self.hidden1(X)\n",
    "        X = self.act1(X)\n",
    "         # second hidden layer\n",
    "        X = self.hidden2(X)\n",
    "        X = self.act2(X)\n",
    "        # third hidden layer and output\n",
    "        X = self.hidden3(X)\n",
    "        X = self.act3(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7459a1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-17T13:21:31.628373Z",
     "start_time": "2022-09-17T13:21:31.611817Z"
    }
   },
   "outputs": [],
   "source": [
    "# train the model\n",
    "def train_model(train_dl, model):\n",
    "    # define the optimization\n",
    "    criterion = CrossEntropyLoss()\n",
    "    optimizer = SGD(model.parameters(), lr=0.01)\n",
    "    # enumerate epochs\n",
    "    for epoch in tqdm_notebook(range(500)):\n",
    "        # enumerate mini batches\n",
    "        for i, (inputs, targets) in enumerate(train_dl):\n",
    "            # clear the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # compute the model output\n",
    "            yhat = model(inputs)\n",
    "            # calculate loss\n",
    "            loss = criterion(yhat, targets)\n",
    "            # credit assignment\n",
    "            loss.backward()\n",
    "            # update model weights\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee6d828d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-17T13:21:31.646233Z",
     "start_time": "2022-09-17T13:21:31.632408Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_model(test_dl, model):\n",
    "    predictions, actuals = list(), list()\n",
    "    for i, (inputs, targets) in enumerate(test_dl):\n",
    "        # evaluate the model on the test set\n",
    "        yhat = model(inputs)\n",
    "        # retrieve numpy array\n",
    "        yhat = yhat.detach().numpy()\n",
    "        actual = targets.numpy()\n",
    "        # convert to class labels\n",
    "        yhat = np.argmax(yhat, axis=1)\n",
    "        # reshape for stacking\n",
    "        actual = actual.reshape(-1, 1)\n",
    "        yhat = yhat.reshape(-1, 1)\n",
    "        # store\n",
    "        predictions.append(yhat)\n",
    "        actuals.append(actual)\n",
    "    predictions, actuals = np.vstack(predictions), np.vstack(actuals)\n",
    "    # calculate accuracy\n",
    "    acc = accuracy_score(actuals, predictions)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c878eabb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-17T13:21:31.660297Z",
     "start_time": "2022-09-17T13:21:31.646719Z"
    }
   },
   "outputs": [],
   "source": [
    "# prepare the data\n",
    "path = 'model_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "caaf7f76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-17T13:21:31.714371Z",
     "start_time": "2022-09-17T13:21:31.661718Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4256 1064\n"
     ]
    }
   ],
   "source": [
    "train_dl, test_dl = prepare_data(path)\n",
    "print(len(train_dl.dataset), len(test_dl.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1aaf04c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-17T13:21:31.743427Z",
     "start_time": "2022-09-17T13:21:31.719939Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1d61143a040>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9001cfff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-17T13:21:31.759660Z",
     "start_time": "2022-09-17T13:21:31.747592Z"
    }
   },
   "outputs": [],
   "source": [
    "# define the netw\n",
    "model = MLP(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "31ec495e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-17T13:21:31.778781Z",
     "start_time": "2022-09-17T13:21:31.764619Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (hidden1): Linear(in_features=12, out_features=10, bias=True)\n",
       "  (act1): ReLU()\n",
       "  (hidden2): Linear(in_features=10, out_features=8, bias=True)\n",
       "  (act2): ReLU()\n",
       "  (hidden3): Linear(in_features=8, out_features=2, bias=True)\n",
       "  (act3): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "43d82326",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-17T13:26:39.994619Z",
     "start_time": "2022-09-17T13:21:31.781686Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d74d326c3860421684a25ce21d29e1a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train the model\n",
    "train_model(train_dl, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d09f1cc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-17T13:26:40.500882Z",
     "start_time": "2022-09-17T13:26:39.998820Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.940\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "acc = evaluate_model(test_dl, model)\n",
    "print('Accuracy: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ff650a",
   "metadata": {},
   "source": [
    "### Todo: Update CSVDataset class to include data cleaning steps. Create your own model and try to run it on different datasets, you can extract some datasets from pytorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71c970c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
